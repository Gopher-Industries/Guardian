{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ff66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, Bidirectional, LSTM, Dropout,\n",
    "                                     BatchNormalization, MultiHeadAttention,\n",
    "                                     GlobalAveragePooling1D, Dense, Add)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecb8320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data shape: (1814, 750, 90)\n",
      "Final labels shape: (1814, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This code for the BiLSTM model with additional outcome metrics\n",
    "\n",
    "from datetime import datetime \n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = 'D:\\Feature Engineering\\Room2_npy'\n",
    "\n",
    "# Define the target length for data trimming/padding\n",
    "target_length = 750\n",
    "\n",
    "# Define parameters for model\n",
    "Model_type = \"Bidirectional Long Short-Term Memory\"\n",
    "batchsize = 32\n",
    "monitor_choice = 'val_loss'\n",
    "dropout = 0.3\n",
    "learningrate = 0.001\n",
    "\n",
    "#Initialise lists\n",
    "data_list = []\n",
    "data_list1 = []\n",
    "labels_list = []\n",
    "\n",
    "classes_to_double = [\"kneel\", \"liedown\", \"pickup\"]\n",
    "\n",
    "# Helper function to split sequences\n",
    "\n",
    "def split_sequence(sequence, step):\n",
    "    return [sequence[i:i + step] for i in range(0, len(sequence), step) if len(sequence[i:i + step]) == step]\n",
    "\n",
    "# Traverse through each activity directory\n",
    "for activity_folder in os.listdir(dataset_path):\n",
    "    activity_path = os.path.join(dataset_path, activity_folder)\n",
    "##    print(f\"Processing activity: {activity_folder}\")\n",
    "\n",
    "    if os.path.isdir(activity_path):\n",
    "        for participant_file in os.listdir(activity_path):\n",
    "            participant_path = os.path.join(activity_path, participant_file)\n",
    "            if os.path.isfile(participant_path) and participant_file.endswith('.npy'):\n",
    "##                print(f\"Loading data from file: {participant_file}\")\n",
    "                data = np.load(participant_path)\n",
    "                data = np.real(data)\n",
    "\n",
    "                # Split data into smaller sequences\n",
    "                data_splits = split_sequence(data, target_length)\n",
    "\n",
    "                for split_data in data_splits:\n",
    "                    # Normalize data\n",
    "#                     min_val = np.min(split_data)\n",
    "#                     max_val = np.max(split_data)\n",
    "                   ##  Z-Score normalization\n",
    "                    mean = np.mean(split_data, axis=0)\n",
    "                    std = np.std(split_data, axis=0)\n",
    "                    data_normalized = (split_data - mean) / (std + 1e-8) \n",
    "\n",
    "                    label = activity_folder\n",
    "#                    data_list1.append(data_normalized[:, 0:30])          \n",
    "#                    labels_list.append(label)\n",
    "#                    data_list1.append(data_normalized[:, 30:60])          \n",
    "#                    labels_list.append(label)\n",
    "                    data_list.append(data_normalized)          \n",
    "                    labels_list.append(label)\n",
    "                    if label in classes_to_double:\n",
    "                        data_list.append(data_normalized.copy())  # Double the data\n",
    "                        labels_list.append(label)  # Double the label\n",
    "\n",
    "# Convert lists to arrays for machine learning processing\n",
    "data = np.array(data_list)\n",
    "labels = np.array(labels_list).reshape(-1, 1)\n",
    "\n",
    "print(\"Final data shape:\", data.shape)\n",
    "print(\"Final labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce79897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape after PCA: (1814, 750, 50)\n"
     ]
    }
   ],
   "source": [
    "## Reshaping the data and applying PCA\n",
    "original_shape = data.shape \n",
    "data_reshaped = data.reshape(-1, original_shape[2])\n",
    "pca = PCA(n_components=50)\n",
    "data_pca_flat = pca.fit_transform(data_reshaped) \n",
    "data_pca = data_pca_flat.reshape(original_shape[0], original_shape[1], -1)\n",
    "print(\"New shape after PCA:\", data_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7f09aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">750</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">750</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">183,296</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">750</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">750</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">750</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,968</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m750\u001b[0m, \u001b[38;5;34m50\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m750\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m183,296\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m750\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m750\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m750\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m131,968\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m33,024\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m520\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">391,496</span> (1.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m391,496\u001b[0m (1.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">390,728</span> (1.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m390,728\u001b[0m (1.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set your learning rate and batch size\n",
    "learningrate = 0.001\n",
    "batchsize = 32\n",
    "epochsvalue = 100\n",
    "patience_number = 20\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "labels_encoded = encoder.fit_transform(labels.reshape(-1, 1))\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(data_pca, labels_encoded, test_size=0.2, random_state=42)\n",
    "# Input layer\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "# BiLSTM layer\n",
    "bilstm_out = Bidirectional(LSTM(128, return_sequences=True), merge_mode='sum')(inputs)\n",
    "drop1 = Dropout(0.3)(bilstm_out)\n",
    "norm1 = BatchNormalization()(drop1)\n",
    "# Attention layer\n",
    "attn_out = MultiHeadAttention(num_heads=4, key_dim=64)(query=norm1, value=norm1, key=norm1)\n",
    "attn_pool = GlobalAveragePooling1D()(attn_out)\n",
    "# DenseNet-style dense stack\n",
    "dense1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(attn_pool)\n",
    "drop2 = Dropout(0.3)(dense1)\n",
    "bn2 = BatchNormalization()(drop2)\n",
    "dense2 = Dense(128, activation='relu')(bn2)\n",
    "drop3 = Dropout(0.3)(dense2)\n",
    "dense3 = Dense(64, activation='relu')(drop3)\n",
    "drop4 = Dropout(0.3)(dense3)\n",
    "# Output layer\n",
    "outputs = Dense(y_train_encoded.shape[1], activation='softmax')(drop4)\n",
    "# Compile model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3145c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 689ms/step - accuracy: 0.3542 - loss: 1.9892 - val_accuracy: 0.5372 - val_loss: 2.1329\n",
      "Epoch 2/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 704ms/step - accuracy: 0.6817 - loss: 1.0829 - val_accuracy: 0.6860 - val_loss: 1.9076\n",
      "Epoch 3/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 762ms/step - accuracy: 0.7821 - loss: 0.7803 - val_accuracy: 0.8457 - val_loss: 1.5305\n",
      "Epoch 4/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 748ms/step - accuracy: 0.8500 - loss: 0.5816 - val_accuracy: 0.9174 - val_loss: 0.9696\n",
      "Epoch 5/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 744ms/step - accuracy: 0.8717 - loss: 0.5390 - val_accuracy: 0.8843 - val_loss: 0.5983\n",
      "Epoch 6/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 733ms/step - accuracy: 0.8938 - loss: 0.4675 - val_accuracy: 0.9008 - val_loss: 0.4749\n",
      "Epoch 7/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 741ms/step - accuracy: 0.8943 - loss: 0.4380 - val_accuracy: 0.9201 - val_loss: 0.3861\n",
      "Epoch 8/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 698ms/step - accuracy: 0.9224 - loss: 0.3694 - val_accuracy: 0.9559 - val_loss: 0.2750\n",
      "Epoch 9/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 688ms/step - accuracy: 0.9231 - loss: 0.3474 - val_accuracy: 0.9421 - val_loss: 0.2982\n",
      "Epoch 10/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 681ms/step - accuracy: 0.9332 - loss: 0.3567 - val_accuracy: 0.9559 - val_loss: 0.2521\n",
      "Epoch 11/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 676ms/step - accuracy: 0.9389 - loss: 0.3189 - val_accuracy: 0.9532 - val_loss: 0.2408\n",
      "Epoch 12/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 683ms/step - accuracy: 0.9452 - loss: 0.3049 - val_accuracy: 0.9559 - val_loss: 0.2552\n",
      "Epoch 13/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 679ms/step - accuracy: 0.9342 - loss: 0.3602 - val_accuracy: 0.9697 - val_loss: 0.2266\n",
      "Epoch 14/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 683ms/step - accuracy: 0.9373 - loss: 0.2930 - val_accuracy: 0.9532 - val_loss: 0.2446\n",
      "Epoch 15/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 682ms/step - accuracy: 0.9436 - loss: 0.3122 - val_accuracy: 0.9642 - val_loss: 0.2498\n",
      "Epoch 16/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 695ms/step - accuracy: 0.9616 - loss: 0.2420 - val_accuracy: 0.9587 - val_loss: 0.2801\n",
      "Epoch 17/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 690ms/step - accuracy: 0.9645 - loss: 0.2306 - val_accuracy: 0.9725 - val_loss: 0.1909\n",
      "Epoch 18/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 685ms/step - accuracy: 0.9594 - loss: 0.2356 - val_accuracy: 0.9752 - val_loss: 0.1729\n",
      "Epoch 19/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 690ms/step - accuracy: 0.9602 - loss: 0.2287 - val_accuracy: 0.9697 - val_loss: 0.1796\n",
      "Epoch 20/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 686ms/step - accuracy: 0.9694 - loss: 0.2247 - val_accuracy: 0.9780 - val_loss: 0.1415\n",
      "Epoch 21/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 688ms/step - accuracy: 0.9751 - loss: 0.1783 - val_accuracy: 0.9669 - val_loss: 0.1603\n",
      "Epoch 22/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 685ms/step - accuracy: 0.9767 - loss: 0.1694 - val_accuracy: 0.9807 - val_loss: 0.1469\n",
      "Epoch 23/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 703ms/step - accuracy: 0.9661 - loss: 0.2063 - val_accuracy: 0.9917 - val_loss: 0.1343\n",
      "Epoch 24/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 694ms/step - accuracy: 0.9828 - loss: 0.1453 - val_accuracy: 0.9835 - val_loss: 0.1404\n",
      "Epoch 25/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 688ms/step - accuracy: 0.9781 - loss: 0.1945 - val_accuracy: 0.9862 - val_loss: 0.1213\n",
      "Epoch 26/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 687ms/step - accuracy: 0.9826 - loss: 0.1450 - val_accuracy: 0.9807 - val_loss: 0.1446\n",
      "Epoch 27/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 691ms/step - accuracy: 0.9830 - loss: 0.1371 - val_accuracy: 0.9807 - val_loss: 0.1350\n",
      "Epoch 28/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 679ms/step - accuracy: 0.9930 - loss: 0.1024 - val_accuracy: 0.9835 - val_loss: 0.1308\n",
      "Epoch 29/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 680ms/step - accuracy: 0.9854 - loss: 0.1392 - val_accuracy: 0.9862 - val_loss: 0.1478\n",
      "Epoch 30/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 678ms/step - accuracy: 0.9846 - loss: 0.1245 - val_accuracy: 0.9917 - val_loss: 0.0989\n",
      "Epoch 31/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 682ms/step - accuracy: 0.9821 - loss: 0.1283 - val_accuracy: 0.9697 - val_loss: 0.2291\n",
      "Epoch 32/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 682ms/step - accuracy: 0.9935 - loss: 0.1047 - val_accuracy: 0.9917 - val_loss: 0.1052\n",
      "Epoch 33/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 679ms/step - accuracy: 0.9973 - loss: 0.0973 - val_accuracy: 0.9862 - val_loss: 0.1583\n",
      "Epoch 34/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 677ms/step - accuracy: 0.9925 - loss: 0.0884 - val_accuracy: 0.9835 - val_loss: 0.1312\n",
      "Epoch 35/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 680ms/step - accuracy: 0.9741 - loss: 0.1895 - val_accuracy: 0.9890 - val_loss: 0.1216\n",
      "Epoch 36/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 678ms/step - accuracy: 0.9828 - loss: 0.1099 - val_accuracy: 0.9669 - val_loss: 0.1545\n",
      "Epoch 37/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 680ms/step - accuracy: 0.9728 - loss: 0.1639 - val_accuracy: 0.9890 - val_loss: 0.1220\n",
      "Epoch 38/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 683ms/step - accuracy: 0.9881 - loss: 0.0982 - val_accuracy: 0.9835 - val_loss: 0.1008\n",
      "Epoch 39/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 676ms/step - accuracy: 0.9828 - loss: 0.1197 - val_accuracy: 0.9669 - val_loss: 0.1514\n",
      "Epoch 40/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 676ms/step - accuracy: 0.9729 - loss: 0.1733 - val_accuracy: 0.9725 - val_loss: 0.1639\n",
      "Epoch 41/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 687ms/step - accuracy: 0.9692 - loss: 0.1659 - val_accuracy: 0.9669 - val_loss: 0.1642\n",
      "Epoch 42/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 691ms/step - accuracy: 0.9689 - loss: 0.1452 - val_accuracy: 0.9807 - val_loss: 0.1020\n",
      "Epoch 43/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 689ms/step - accuracy: 0.9874 - loss: 0.0954 - val_accuracy: 0.9917 - val_loss: 0.0954\n",
      "Epoch 44/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 687ms/step - accuracy: 0.9977 - loss: 0.0643 - val_accuracy: 0.9972 - val_loss: 0.0781\n",
      "Epoch 45/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 687ms/step - accuracy: 0.9962 - loss: 0.0619 - val_accuracy: 0.9862 - val_loss: 0.1054\n",
      "Epoch 46/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 695ms/step - accuracy: 0.9837 - loss: 0.1125 - val_accuracy: 0.9780 - val_loss: 0.0933\n",
      "Epoch 47/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 689ms/step - accuracy: 0.9881 - loss: 0.0932 - val_accuracy: 0.9835 - val_loss: 0.1011\n",
      "Epoch 48/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 683ms/step - accuracy: 0.9804 - loss: 0.1017 - val_accuracy: 0.9807 - val_loss: 0.1298\n",
      "Epoch 49/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 681ms/step - accuracy: 0.9911 - loss: 0.0715 - val_accuracy: 0.9972 - val_loss: 0.0791\n",
      "Epoch 50/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 678ms/step - accuracy: 0.9954 - loss: 0.0869 - val_accuracy: 0.9835 - val_loss: 0.0940\n",
      "Epoch 51/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 677ms/step - accuracy: 0.9879 - loss: 0.0746 - val_accuracy: 0.9972 - val_loss: 0.0663\n",
      "Epoch 52/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 677ms/step - accuracy: 0.9920 - loss: 0.0671 - val_accuracy: 0.9752 - val_loss: 0.1306\n",
      "Epoch 53/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 691ms/step - accuracy: 0.9910 - loss: 0.0743 - val_accuracy: 0.9890 - val_loss: 0.0656\n",
      "Epoch 54/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 689ms/step - accuracy: 0.9870 - loss: 0.0908 - val_accuracy: 0.9780 - val_loss: 0.1014\n",
      "Epoch 55/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 691ms/step - accuracy: 0.9850 - loss: 0.1354 - val_accuracy: 0.9890 - val_loss: 0.0617\n",
      "Epoch 56/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 699ms/step - accuracy: 0.9913 - loss: 0.0650 - val_accuracy: 0.9780 - val_loss: 0.1203\n",
      "Epoch 57/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 689ms/step - accuracy: 0.9993 - loss: 0.0445 - val_accuracy: 0.9807 - val_loss: 0.1279\n",
      "Epoch 58/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 689ms/step - accuracy: 0.9961 - loss: 0.0642 - val_accuracy: 0.9807 - val_loss: 0.1042\n",
      "Epoch 59/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 682ms/step - accuracy: 0.9910 - loss: 0.0528 - val_accuracy: 0.9807 - val_loss: 0.0836\n",
      "Epoch 60/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 680ms/step - accuracy: 0.9997 - loss: 0.0372 - val_accuracy: 0.9807 - val_loss: 0.0825\n",
      "Epoch 61/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 678ms/step - accuracy: 0.9983 - loss: 0.0363 - val_accuracy: 0.9862 - val_loss: 0.0962\n",
      "Epoch 62/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 678ms/step - accuracy: 0.9815 - loss: 0.1532 - val_accuracy: 0.9642 - val_loss: 0.1245\n",
      "Epoch 63/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 691ms/step - accuracy: 0.9874 - loss: 0.0697 - val_accuracy: 0.9669 - val_loss: 0.1215\n",
      "Epoch 64/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 680ms/step - accuracy: 0.9904 - loss: 0.0629 - val_accuracy: 0.9917 - val_loss: 0.0513\n",
      "Epoch 65/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 681ms/step - accuracy: 0.9925 - loss: 0.0582 - val_accuracy: 0.9862 - val_loss: 0.0862\n",
      "Epoch 66/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 676ms/step - accuracy: 0.9946 - loss: 0.0471 - val_accuracy: 0.9862 - val_loss: 0.0609\n",
      "Epoch 67/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 675ms/step - accuracy: 0.9991 - loss: 0.0364 - val_accuracy: 0.9835 - val_loss: 0.0550\n",
      "Epoch 68/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 680ms/step - accuracy: 0.9927 - loss: 0.0525 - val_accuracy: 0.9669 - val_loss: 0.2289\n",
      "Epoch 69/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 679ms/step - accuracy: 0.9749 - loss: 0.1353 - val_accuracy: 0.9752 - val_loss: 0.0963\n",
      "Epoch 70/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 679ms/step - accuracy: 0.9971 - loss: 0.0364 - val_accuracy: 0.9752 - val_loss: 0.1063\n",
      "Epoch 71/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 676ms/step - accuracy: 0.9945 - loss: 0.0450 - val_accuracy: 0.9917 - val_loss: 0.0571\n",
      "Epoch 72/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 674ms/step - accuracy: 0.9925 - loss: 0.0422 - val_accuracy: 0.9917 - val_loss: 0.0462\n",
      "Epoch 73/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 689ms/step - accuracy: 0.9950 - loss: 0.0651 - val_accuracy: 0.9697 - val_loss: 0.0826\n",
      "Epoch 74/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 677ms/step - accuracy: 0.9854 - loss: 0.0855 - val_accuracy: 0.9862 - val_loss: 0.0703\n",
      "Epoch 75/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 686ms/step - accuracy: 0.9872 - loss: 0.0713 - val_accuracy: 0.9835 - val_loss: 0.0999\n",
      "Epoch 76/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 694ms/step - accuracy: 0.9890 - loss: 0.0544 - val_accuracy: 0.9917 - val_loss: 0.0716\n",
      "Epoch 77/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 686ms/step - accuracy: 0.9932 - loss: 0.0510 - val_accuracy: 0.9780 - val_loss: 0.1003\n",
      "Epoch 78/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 693ms/step - accuracy: 0.9948 - loss: 0.0500 - val_accuracy: 0.9917 - val_loss: 0.0681\n",
      "Epoch 79/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 694ms/step - accuracy: 0.9977 - loss: 0.0306 - val_accuracy: 0.9752 - val_loss: 0.0977\n",
      "Epoch 80/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 693ms/step - accuracy: 0.9949 - loss: 0.0360 - val_accuracy: 0.9835 - val_loss: 0.0610\n",
      "Epoch 81/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 692ms/step - accuracy: 0.9969 - loss: 0.0327 - val_accuracy: 0.9890 - val_loss: 0.0528\n",
      "Epoch 82/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 694ms/step - accuracy: 0.9951 - loss: 0.0381 - val_accuracy: 0.9862 - val_loss: 0.0788\n",
      "Epoch 83/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 699ms/step - accuracy: 0.9995 - loss: 0.0267 - val_accuracy: 0.9917 - val_loss: 0.0622\n",
      "Epoch 84/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 694ms/step - accuracy: 1.0000 - loss: 0.0217 - val_accuracy: 0.9917 - val_loss: 0.0634\n",
      "Epoch 85/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 692ms/step - accuracy: 0.9786 - loss: 0.1290 - val_accuracy: 0.9669 - val_loss: 0.1384\n",
      "Epoch 86/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 694ms/step - accuracy: 0.9881 - loss: 0.0650 - val_accuracy: 0.9752 - val_loss: 0.0872\n",
      "Epoch 87/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 690ms/step - accuracy: 0.9898 - loss: 0.0690 - val_accuracy: 0.9725 - val_loss: 0.1206\n",
      "Epoch 88/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 697ms/step - accuracy: 0.9912 - loss: 0.0446 - val_accuracy: 0.9807 - val_loss: 0.0803\n",
      "Epoch 89/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 696ms/step - accuracy: 0.9928 - loss: 0.0554 - val_accuracy: 0.9862 - val_loss: 0.0831\n",
      "Epoch 90/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 682ms/step - accuracy: 0.9856 - loss: 0.0931 - val_accuracy: 0.9669 - val_loss: 0.1226\n",
      "Epoch 91/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 682ms/step - accuracy: 0.9906 - loss: 0.0491 - val_accuracy: 0.9972 - val_loss: 0.0327\n",
      "Epoch 92/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 683ms/step - accuracy: 0.9921 - loss: 0.0462 - val_accuracy: 0.9890 - val_loss: 0.0441\n",
      "Epoch 93/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 680ms/step - accuracy: 0.9946 - loss: 0.0324 - val_accuracy: 0.9862 - val_loss: 0.0470\n",
      "Epoch 94/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 685ms/step - accuracy: 0.9977 - loss: 0.0273 - val_accuracy: 0.9835 - val_loss: 0.0487\n",
      "Epoch 95/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 684ms/step - accuracy: 0.9905 - loss: 0.0546 - val_accuracy: 0.9972 - val_loss: 0.0314\n",
      "Epoch 96/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 687ms/step - accuracy: 0.9986 - loss: 0.0281 - val_accuracy: 0.9917 - val_loss: 0.0409\n",
      "Epoch 97/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 678ms/step - accuracy: 1.0000 - loss: 0.0211 - val_accuracy: 0.9945 - val_loss: 0.0337\n",
      "Epoch 98/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 680ms/step - accuracy: 0.9991 - loss: 0.0197 - val_accuracy: 0.9972 - val_loss: 0.0304\n",
      "Epoch 99/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 679ms/step - accuracy: 0.9972 - loss: 0.0268 - val_accuracy: 0.9862 - val_loss: 0.0476\n",
      "Epoch 100/100\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 676ms/step - accuracy: 0.9915 - loss: 0.0511 - val_accuracy: 0.9835 - val_loss: 0.0676\n",
      "Final training loss: 0.0631\n",
      "Final training accuracy: 0.9897\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 195ms/step - accuracy: 0.9923 - loss: 0.0474\n",
      "Test loss: 0.0304\n",
      "Test accuracy: 0.9972\n"
     ]
    }
   ],
   "source": [
    "# Implement early stopping\n",
    "monitor_choice = 'val_loss'\n",
    "patience_number = 20\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "epochsvalue = 100\n",
    "\n",
    "# start the timer for model training\n",
    "start_time = datetime.now() \n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded, epochs=epochsvalue, batch_size=32, validation_data=(X_test, y_test_encoded), callbacks=[early_stopping])\n",
    "\n",
    "# Get the final training accuracy and loss\n",
    "train_loss = history.history['loss'][-1]\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "print(f\"Final training loss: {train_loss:.4f}\")\n",
    "print(f\"Final training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#End timing and calculate total time for training\n",
    "end_time = datetime.now() \n",
    "time_difference = (end_time - start_time).total_seconds()/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88dd4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
